{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c6525-4ea6-4d5b-a938-ce189adfc47f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf15f7-0ca2-433d-95ff-fcc3a51b63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# 路径配置\n",
    "# ----------------------------\n",
    "DATA_DIR = \"./data\"\n",
    "IMG_TRAIN_DIR = os.path.join(DATA_DIR, \"Image_arrays_train\")\n",
    "IMG_TEST_DIR = os.path.join(DATA_DIR, \"Image_arrays_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82aa27-6990-48d6-8d92-922285c66308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(field_id, is_train=True, normalize=True):\n",
    "    folder = IMG_TRAIN_DIR if is_train else IMG_TEST_DIR\n",
    "    path = os.path.join(folder, f\"{field_id}.npy\")\n",
    "    if not os.path.exists(path):\n",
    "        return np.zeros((360, 41, 41), dtype=np.float32)\n",
    "    img = np.load(path).astype(np.float32)\n",
    "    if normalize:\n",
    "        min_val, max_val = img.min(), img.max()\n",
    "        if max_val > min_val:\n",
    "            img = (img - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            img = np.zeros_like(img)\n",
    "    return img\n",
    "\n",
    "def get_aux_features(field_id, soil_climate_df):\n",
    "    if field_id in soil_climate_df.index:\n",
    "        return soil_climate_df.loc[field_id].values.astype(np.float32)\n",
    "    return np.zeros(soil_climate_df.shape[1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f66c6-8a53-4dee-84d5-0d161be6b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropYieldDataset(Dataset):\n",
    "    def __init__(self, df, soil_climate, is_train=True, aux_scaler=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.soil_climate = soil_climate\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.images = []\n",
    "        self.aux_feats = []\n",
    "        self.targets = []\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            fid = row[\"Field_ID\"]\n",
    "            self.images.append(load_image(fid, is_train))\n",
    "            self.aux_feats.append(get_aux_features(fid, soil_climate))\n",
    "            if is_train:\n",
    "                self.targets.append(row[\"Yield\"])\n",
    "\n",
    "        self.images = np.stack(self.images)  # (N, 360, 41, 41)\n",
    "        self.aux_feats = np.stack(self.aux_feats)  # (N, D_aux)\n",
    "\n",
    "        if aux_scaler is None:\n",
    "            self.aux_scaler = StandardScaler()\n",
    "            self.aux_feats = self.aux_scaler.fit_transform(self.aux_feats)\n",
    "        else:\n",
    "            self.aux_feats = aux_scaler.transform(self.aux_feats)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.tensor(self.images[idx])  # (360, 41, 41)\n",
    "        aux = torch.tensor(self.aux_feats[idx])  # (D_aux,)\n",
    "        if self.is_train:\n",
    "            y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return img, aux, y\n",
    "        return img, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cab18a-72d3-4626-b5cf-fa366bab2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AW_MIL_CropYield(nn.Module):\n",
    "    def __init__(self, aux_dim, img_channels=1, embed_dim=128, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # 图像编码器：1D CNN over time\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=img_channels * 41 * 41, out_channels=64, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64, out_channels=embed_dim, kernel_size=5, padding=2),\n",
    "            nn.AdaptiveAvgPool1d(1)  # (B, E, 1)\n",
    "        )\n",
    "\n",
    "        # 辅助特征投影\n",
    "        self.aux_proj = nn.Linear(aux_dim, embed_dim)\n",
    "\n",
    "        # Attention weights for instances (here each field is one instance)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        # 回归头\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, aux):\n",
    "        B = img.shape[0]\n",
    "        # Reshape image: (B, T, H, W) -> (B, C*H*W, T)\n",
    "        img = img.unsqueeze(1)  # (B, 1, 360, 41, 41)\n",
    "        img = img.view(B, 1 * 41 * 41, 360)  # (B, 1681, 360)\n",
    "\n",
    "        # Temporal encoding\n",
    "        img_emb = self.temporal_conv(img).squeeze(-1)  # (B, E)\n",
    "\n",
    "        # Aux encoding\n",
    "        aux_emb = self.aux_proj(aux)  # (B, E)\n",
    "\n",
    "        # Concatenate\n",
    "        combined = torch.cat([img_emb, aux_emb], dim=1)  # (B, 2E)\n",
    "\n",
    "        # Attention (for MIL; here trivial since 1 instance per bag)\n",
    "        att_weights = self.attention(combined)  # (B, 1)\n",
    "        att_weights = torch.softmax(att_weights, dim=0)  # across instances (if multiple)\n",
    "\n",
    "        # Weighted sum (still valid for single instance)\n",
    "        weighted_emb = att_weights * combined  # (B, 2E)\n",
    "\n",
    "        # Regression\n",
    "        out = self.regressor(weighted_emb).squeeze(-1)  # (B,)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc493a63-b352-49f6-bf89-617d4096e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    train_df = pd.read_csv(os.path.join(DATA_DIR, \"Train.csv\"), header=None)\n",
    "    train_df.columns = [\"Field_ID\", \"Year\", \"Quality\", \"Yield\"]\n",
    "    test_df = pd.read_csv(os.path.join(DATA_DIR, \"test_field_ids_with_year.csv\"))\n",
    "    soil_climate = pd.read_excel(os.path.join(DATA_DIR, \"samply.xlsx\"), engine='openpyxl')\n",
    "    soil_climate.set_index(\"Field_ID\", inplace=True)\n",
    "\n",
    "    # Train/Val split\n",
    "    train_meta, val_meta = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = CropYieldDataset(train_meta, soil_climate, is_train=True)\n",
    "    val_dataset = CropYieldDataset(val_meta, soil_climate, is_train=True, aux_scaler=train_dataset.aux_scaler)\n",
    "    test_dataset = CropYieldDataset(test_df, soil_climate, is_train=False, aux_scaler=train_dataset.aux_scaler)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AW_MIL_CropYield(aux_dim=train_dataset.aux_feats.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "   # Training loop\n",
    "    best_val_rmse = float('inf')\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mse = 0.0\n",
    "        for img, aux, y in train_loader:\n",
    "            img, aux, y = img.to(device), aux.to(device), y.to(device)\n",
    "            pred = model(img, aux)\n",
    "            loss = criterion(pred, y)  # MSE loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_mse += loss.item()  # since criterion is MSE\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_mse = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img, aux, y in val_loader:\n",
    "                img, aux, y = img.to(device), aux.to(device), y.to(device)\n",
    "                pred = model(img, aux)\n",
    "                val_mse += criterion(pred, y).item()\n",
    "\n",
    "        # Compute averages\n",
    "        train_mse /= len(train_loader)\n",
    "        val_mse /= len(val_loader)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:2d} | \"\n",
    "              f\"Train MSE: {train_mse:.4f} | Train RMSE: {train_rmse:.4f} | \"\n",
    "              f\"Val MSE: {val_mse:.4f} | Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "        # Save best model based on RMSE\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            torch.save(model.state_dict(), os.path.join(DATA_DIR, \"best_model.pth\"))\n",
    "            print(f\"New best RMSE: {best_val_rmse:.4f} — model saved.\")\n",
    "\n",
    "    # Test prediction\n",
    "    model.load_state_dict(torch.load(os.path.join(DATA_DIR, \"best_model.pth\")))\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for img, aux in test_loader:\n",
    "            img, aux = img.to(device), aux.to(device)\n",
    "            pred = model(img, aux)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "\n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        \"Field_ID\": test_df[\"Field_ID\"],\n",
    "        \"Yield\": np.clip(preds, 0, None)  # 防止负产量\n",
    "    })\n",
    "    submission.to_csv(os.path.join(DATA_DIR, \"submission_AW_MIL.csv\"), index=False)\n",
    "    print(\"✅ Submission saved to submission_AW_MIL.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
